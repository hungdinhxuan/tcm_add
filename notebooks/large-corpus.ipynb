{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>subset</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SNS/2024/Zuckerberg_38.wav</td>\n",
       "      <td>train</td>\n",
       "      <td>bonafide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SNS/2024/Matt_damon_58.wav</td>\n",
       "      <td>train</td>\n",
       "      <td>bonafide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SNS/2024/Anne_Hathaway_83.wav</td>\n",
       "      <td>train</td>\n",
       "      <td>bonafide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SNS/2024/Zuckerberg_127.wav</td>\n",
       "      <td>train</td>\n",
       "      <td>bonafide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SNS/2024/Macron_193.wav</td>\n",
       "      <td>train</td>\n",
       "      <td>bonafide</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       file_name subset     label\n",
       "0     SNS/2024/Zuckerberg_38.wav  train  bonafide\n",
       "1     SNS/2024/Matt_damon_58.wav  train  bonafide\n",
       "2  SNS/2024/Anne_Hathaway_83.wav  train  bonafide\n",
       "3    SNS/2024/Zuckerberg_127.wav  train  bonafide\n",
       "4        SNS/2024/Macron_193.wav  train  bonafide"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "\n",
    "LARGE_CORPUS_FOLDER = '../0_large-corpus'\n",
    "original_protocol_file_path = os.path.join(LARGE_CORPUS_FOLDER, 'protocol.txt')\n",
    "df = pd.read_csv(original_protocol_file_path, sep=' ', header=None)\n",
    "df.columns = ['file_name', 'subset', 'label']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add more librosa trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>subset</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trim_librosa/SNS/2024/Zuckerberg_38.wav</td>\n",
       "      <td>train</td>\n",
       "      <td>bonafide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trim_librosa/SNS/2024/Matt_damon_58.wav</td>\n",
       "      <td>train</td>\n",
       "      <td>bonafide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trim_librosa/SNS/2024/Anne_Hathaway_83.wav</td>\n",
       "      <td>train</td>\n",
       "      <td>bonafide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trim_librosa/SNS/2024/Zuckerberg_127.wav</td>\n",
       "      <td>train</td>\n",
       "      <td>bonafide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trim_librosa/SNS/2024/Macron_193.wav</td>\n",
       "      <td>train</td>\n",
       "      <td>bonafide</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    file_name subset     label\n",
       "0     trim_librosa/SNS/2024/Zuckerberg_38.wav  train  bonafide\n",
       "1     trim_librosa/SNS/2024/Matt_damon_58.wav  train  bonafide\n",
       "2  trim_librosa/SNS/2024/Anne_Hathaway_83.wav  train  bonafide\n",
       "3    trim_librosa/SNS/2024/Zuckerberg_127.wav  train  bonafide\n",
       "4        trim_librosa/SNS/2024/Macron_193.wav  train  bonafide"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trimmed_librosa = df.copy()\n",
    "\n",
    "TRIMMED_LIBROSA_FOLDER = 'trim_librosa'\n",
    "\n",
    "df_trimmed_librosa['file_name'] = df_trimmed_librosa['file_name'].apply(lambda x: os.path.join(TRIMMED_LIBROSA_FOLDER, x))\n",
    "\n",
    "df_trimmed_librosa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore vocoded files in eval set\n",
    "df_trimmed_librosa = df_trimmed_librosa[~((df_trimmed_librosa[\"file_name\"].str.startswith(\"trim_librosa\")) & (df_trimmed_librosa[\"subset\"] == \"dev\"))]\n",
    "df_trimmed_librosa = df_trimmed_librosa[~((df_trimmed_librosa[\"file_name\"].str.startswith(\"trim_librosa\")) & (df_trimmed_librosa[\"subset\"] == \"eval\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>subset</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>218159</td>\n",
       "      <td>218159</td>\n",
       "      <td>218159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>218159</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>trim_librosa/Real/ASVspoof5/T_0000020664.wav</td>\n",
       "      <td>eval</td>\n",
       "      <td>spoof</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>134441</td>\n",
       "      <td>120695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_name  subset   label\n",
       "count                                         218159  218159  218159\n",
       "unique                                        218159       3       2\n",
       "top     trim_librosa/Real/ASVspoof5/T_0000020664.wav    eval   spoof\n",
       "freq                                               1  134441  120695"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatemate df_trimmed_librosa and df\n",
    "df_concat = pd.concat([df, df_trimmed_librosa], axis=0)\n",
    "\n",
    "df_concat.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df_trimmed_librosa\n",
    "df_trimmed_librosa.to_csv(os.path.join(LARGE_CORPUS_FOLDER, 'protocol_trimmed_librosa_v2.txt'), sep=' ', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9850 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9850/9850 [00:00<00:00, 33045.51it/s]\n",
      "100%|██████████| 9787/9787 [00:00<00:00, 29515.01it/s]\n",
      "100%|██████████| 19023/19023 [00:00<00:00, 38587.49it/s]\n",
      "100%|██████████| 16185/16185 [00:00<00:00, 34050.36it/s]\n",
      "100%|██████████| 134441/134441 [00:03<00:00, 37602.80it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "with open('./scp_librosa_trim/bonafide_train.lst', 'w') as f:\n",
    "    real_train = df_trimmed_librosa[(df_trimmed_librosa['subset'] == 'train') &\n",
    "                         (df_trimmed_librosa['label'] == 'bonafide')]\n",
    "    for index, row in tqdm(real_train.iterrows(), total=real_train.shape[0]):\n",
    "        f.write(f'{row[\"file_name\"]}\\n')\n",
    "        # shutil.copy(row['path'], 'vocoded/')\n",
    "\n",
    "with open('./scp_librosa_trim/bonafide_dev.lst', 'w') as f:\n",
    "    real_dev = df_trimmed_librosa[(df_trimmed_librosa['subset'] == 'dev') &\n",
    "                       (df_trimmed_librosa['label'] == 'bonafide')]\n",
    "    for index, row in tqdm(real_dev.iterrows(), total=real_dev.shape[0]):\n",
    "        f.write(f'{row[\"file_name\"]}\\n')\n",
    "        # shutil.copy(row['path'], 'vocoded/')\n",
    "\n",
    "with open('./scp_librosa_trim/spoof_train.lst', 'w') as f:\n",
    "    real_train = df_trimmed_librosa[(df_trimmed_librosa['subset'] == 'train') &\n",
    "                         (df_trimmed_librosa['label'] == 'spoof')]\n",
    "    for index, row in tqdm(real_train.iterrows(), total=real_train.shape[0]):\n",
    "        f.write(f'{row[\"file_name\"]}\\n')\n",
    "\n",
    "\n",
    "with open('./scp_librosa_trim/spoof_dev.lst', 'w') as f:\n",
    "    real_dev = df_trimmed_librosa[(df_trimmed_librosa['subset'] == 'dev') &\n",
    "                       (df_trimmed_librosa['label'] == 'spoof')]\n",
    "    for index, row in tqdm(real_dev.iterrows(), total=real_dev.shape[0]):\n",
    "        f.write(f'{row[\"file_name\"]}\\n')\n",
    "\n",
    "\n",
    "with open('./scp_librosa_trim/eval.lst', 'w') as f:\n",
    "    full_eval = df_trimmed_librosa[df_trimmed_librosa['subset'] == 'eval']\n",
    "    for index, row in tqdm(full_eval.iterrows(), total=full_eval.shape[0]):\n",
    "        f.write(f'{row[\"file_name\"]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add VAD trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading protocol file...\n",
      "Scanning trim folder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning directories: 100%|██████████| 367/367 [00:03<00:00, 110.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing files using 20 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file chunks: 100%|██████████| 20/20 [00:01<00:00, 14.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new protocol entries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating entries: 100%|██████████| 189286/189286 [01:00<00:00, 3134.22it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating final DataFrame...\n",
      "Original protocol size: 189286\n",
      "New protocol size: 1891235\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_base_filename(filename):\n",
    "    # Extract base filename (number before '___')\n",
    "    match = re.search(r'(\\d+)(?:___\\d+.*)?\\.wav$', os.path.basename(filename))\n",
    "    if match:\n",
    "        return f\"{os.path.dirname(filename)}/{match.group(1)}.wav\"\n",
    "    return filename\n",
    "\n",
    "\n",
    "def is_speech_segment(filename):\n",
    "    # Check if the file is a speech segment (no 'residual' or 'no_speech' in name)\n",
    "    basename = os.path.basename(filename)\n",
    "    if '___' not in basename:\n",
    "        return True\n",
    "    return 'residual' not in basename and 'no_speech' not in basename\n",
    "\n",
    "\n",
    "def process_file_chunk(files, trim_folder):\n",
    "    speech_segments = {}\n",
    "    for file in files:\n",
    "        if is_speech_segment(file):\n",
    "            base_file = get_base_filename(file)\n",
    "            if base_file not in speech_segments:\n",
    "                speech_segments[base_file] = []\n",
    "            speech_segments[base_file].append(file)\n",
    "    return speech_segments\n",
    "\n",
    "\n",
    "def create_new_protocol(meta_data_path, trim_folder, n_workers=None):\n",
    "    if n_workers is None:\n",
    "        n_workers = cpu_count() - 1  # Leave one CPU free\n",
    "\n",
    "    # Read original protocol\n",
    "    print(\"Reading protocol file...\")\n",
    "    meta_data = pd.read_csv(meta_data_path, sep=' ', header=None)\n",
    "    meta_data.columns = ['file_name', 'subset', 'label']\n",
    "\n",
    "    # Get all files from trim folder\n",
    "    print(\"Scanning trim folder...\")\n",
    "    all_files = []\n",
    "    for root, _, files in tqdm(list(os.walk(trim_folder)), desc=\"Scanning directories\"):\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                all_files.append(os.path.relpath(\n",
    "                    os.path.join(root, file), trim_folder))\n",
    "\n",
    "    # Split files into chunks for parallel processing\n",
    "    chunk_size = len(all_files) // n_workers + 1\n",
    "    file_chunks = np.array_split(all_files, n_workers)\n",
    "\n",
    "    # Process chunks in parallel\n",
    "    print(f\"Processing files using {n_workers} workers...\")\n",
    "    with Pool(n_workers) as pool:\n",
    "        partial_process = partial(process_file_chunk, trim_folder=trim_folder)\n",
    "        results = list(tqdm(\n",
    "            pool.imap(partial_process, file_chunks),\n",
    "            total=len(file_chunks),\n",
    "            desc=\"Processing file chunks\"\n",
    "        ))\n",
    "\n",
    "    # Merge results from all workers\n",
    "    speech_segments = {}\n",
    "    for result in results:\n",
    "        for base_file, segments in result.items():\n",
    "            if base_file not in speech_segments:\n",
    "                speech_segments[base_file] = []\n",
    "            speech_segments[base_file].extend(segments)\n",
    "\n",
    "    # Create new protocol entries\n",
    "    print(\"Creating new protocol entries...\")\n",
    "    new_entries = []\n",
    "    for _, row in tqdm(meta_data.iterrows(), total=len(meta_data), desc=\"Creating entries\"):\n",
    "        original_file = row['file_name']\n",
    "        base_file = get_base_filename(original_file)\n",
    "\n",
    "        if base_file in speech_segments:\n",
    "            for segment in speech_segments[base_file]:\n",
    "                new_entries.append({\n",
    "                    'file_name': segment,\n",
    "                    'subset': row['subset'],\n",
    "                    'label': row['label']\n",
    "                })\n",
    "\n",
    "    # Create new protocol DataFrame\n",
    "    print(\"Creating final DataFrame...\")\n",
    "    new_protocol = pd.DataFrame(new_entries)\n",
    "    print(f\"Original protocol size: {len(meta_data)}\")\n",
    "    print(f\"New protocol size: {len(new_protocol)}\")\n",
    "    return new_protocol\n",
    "\n",
    "\n",
    "# Usage example\n",
    "meta_data_path = original_protocol_file_path\n",
    "trim_folder = os.path.join(LARGE_CORPUS_FOLDER, 'trim')\n",
    "\n",
    "# You can specify the number of workers, or let it use (CPU cores - 1)\n",
    "new_protocol = create_new_protocol(meta_data_path, trim_folder, n_workers=20)\n",
    "new_protocol.head()\n",
    "# new_protocol.to_csv('trim_protocol.txt', sep='\\t', index=False)\n",
    "new_protocol['file_name'] = new_protocol['file_name'].apply(lambda x: os.path.join('trim', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subset  label   \n",
       "dev     bonafide     19574\n",
       "        spoof        32370\n",
       "eval    bonafide    135954\n",
       "        spoof       132928\n",
       "train   bonafide     19700\n",
       "        spoof        38046\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat.groupby(['subset', 'label']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatemate df_trimmed_librosa and df\n",
    "df_concat2 = pd.concat([df_concat, new_protocol], axis=0)\n",
    "\n",
    "# save df_concat2\n",
    "df_concat2.to_csv('new_protocol.txt', sep=' ', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subset  label   \n",
       "dev     bonafide     1784317\n",
       "        spoof         141749\n",
       "eval    bonafide    14488209\n",
       "        spoof         909166\n",
       "train   bonafide     1792113\n",
       "        spoof         175372\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_concat2 = pd.read_csv('new_protocol.txt', sep=' ', header=None)\n",
    "df_concat2.columns = ['file_name', 'subset', 'label']\n",
    "\n",
    "# Check number of spoof and bonafide samples each subset\n",
    "df_concat2.groupby(['subset', 'label']).size()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocoder add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning vocoded folders...\n",
      "\n",
      "Processing hifigan...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning directories: 100%|██████████| 1/1 [00:00<00:00,  5.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files found for hifigan: 150591\n",
      "Randomly selecting 10000 files from 150591 files\n",
      "\n",
      "Processing hn-sinc-nsf-hifi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning directories: 100%|██████████| 1/1 [00:00<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files found for hn-sinc-nsf-hifi: 150591\n",
      "Randomly selecting 10000 files from 150591 files\n",
      "\n",
      "Processing waveglow...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning directories: 100%|██████████| 1/1 [00:00<00:00,  6.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files found for waveglow: 150591\n",
      "Randomly selecting 10000 files from 150591 files\n",
      "\n",
      "Example paths from each vocoder:\n",
      "\n",
      "hifigan first 5 files:\n",
      "  vocoded/hifigan/01FHSH0028_00212.wav\n",
      "  vocoded/hifigan/01FHSH0028_00274.wav\n",
      "  vocoded/hifigan/01FHSH0028_00309.wav\n",
      "  vocoded/hifigan/01FHSH0028_00353.wav\n",
      "  vocoded/hifigan/01FHSH0028_00450.wav\n",
      "\n",
      "hn-sinc-nsf-hifi first 5 files:\n",
      "  vocoded/hn-sinc-nsf-hifi/01FHSH0028_00221.wav\n",
      "  vocoded/hn-sinc-nsf-hifi/01FHSH0028_00397.wav\n",
      "  vocoded/hn-sinc-nsf-hifi/01FHSH0028_00684.wav\n",
      "  vocoded/hn-sinc-nsf-hifi/01MLD00033_00763.wav\n",
      "  vocoded/hn-sinc-nsf-hifi/01MLD00033_00948.wav\n",
      "\n",
      "waveglow first 5 files:\n",
      "  vocoded/waveglow/01FHSH0028_00240.wav\n",
      "  vocoded/waveglow/01FHSH0028_00263.wav\n",
      "  vocoded/waveglow/01MPJH0028_00196.wav\n",
      "  vocoded/waveglow/01MPJH0028_00213.wav\n",
      "  vocoded/waveglow/01MPJH0028_00258.wav\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['vocoded/hifigan/01FHSH0028_00212.wav',\n",
       " 'vocoded/hifigan/01FHSH0028_00274.wav',\n",
       " 'vocoded/hifigan/01FHSH0028_00309.wav',\n",
       " 'vocoded/hifigan/01FHSH0028_00353.wav',\n",
       " 'vocoded/hifigan/01FHSH0028_00450.wav',\n",
       " 'vocoded/hifigan/01FHSH0028_00782.wav',\n",
       " 'vocoded/hifigan/01FHSH0028_01001.wav',\n",
       " 'vocoded/hifigan/01MLD00033_00983.wav',\n",
       " 'vocoded/hifigan/01MPJH0028_00196.wav',\n",
       " 'vocoded/hifigan/01MPJH0028_00258.wav']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "vocoded_list = ['hifigan', 'hn-sinc-nsf-hifi', 'waveglow']\n",
    "vocoded_dict = {}\n",
    "\n",
    "print(\"Scanning vocoded folders...\")\n",
    "\n",
    "for vocoder in vocoded_list:\n",
    "    print(f\"\\nProcessing {vocoder}...\")\n",
    "    vocoder_folder = os.path.join(LARGE_CORPUS_FOLDER, 'vocoded', vocoder)\n",
    "    all_files = []\n",
    "\n",
    "    # Collect all files with correct relative paths\n",
    "    for root, _, files in tqdm(list(os.walk(vocoder_folder)), desc=\"Scanning directories\"):\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                # Create path in format: vocoded/<vocoder_type>/filename.wav\n",
    "                file_path = os.path.join('vocoded', vocoder, file)\n",
    "                all_files.append(file_path)\n",
    "\n",
    "    print(f\"Total files found for {vocoder}: {len(all_files)}\")\n",
    "\n",
    "    if len(all_files) > 10000:\n",
    "        print(f\"Randomly selecting 10000 files from {len(all_files)} files\")\n",
    "        all_files = np.random.choice(all_files, 10000, replace=False)\n",
    "    else:\n",
    "        print(f\"Using all {len(all_files)} files (less than 10000)\")\n",
    "\n",
    "    vocoded_dict[vocoder] = sorted(all_files)  # Sort for consistency\n",
    "\n",
    "# Print example paths for verification\n",
    "print(\"\\nExample paths from each vocoder:\")\n",
    "for vocoder in vocoded_list:\n",
    "    print(f\"\\n{vocoder} first 5 files:\")\n",
    "    for file in vocoded_dict[vocoder][:5]:\n",
    "        print(f\"  {file}\")\n",
    "\n",
    "# Return first 10 files from hifigan for the original request\n",
    "vocoded_dict['hifigan'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new protocol entries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing hifigan: 100%|██████████| 10000/10000 [00:00<00:00, 1463061.25it/s]\n",
      "Processing hn-sinc-nsf-hifi: 100%|██████████| 10000/10000 [00:00<00:00, 1650780.86it/s]\n",
      "Processing waveglow: 100%|██████████| 10000/10000 [00:00<00:00, 1680208.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating final DataFrame...\n",
      "Original protocol size: 218159\n",
      "New protocol size: 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Randomly pick 50% of the files from each vocoder for train 50% for dev \n",
    "# label them as spoof and concat them with df_concat\n",
    "\n",
    "# Create new protocol entries\n",
    "print(\"Creating new protocol entries...\")\n",
    "new_entries = []\n",
    "for vocoder, files in vocoded_dict.items():\n",
    "    for file in tqdm(files, desc=f\"Processing {vocoder}\"):\n",
    "        new_entries.append({\n",
    "            'file_name': file,\n",
    "            'subset': 'train',\n",
    "            'label': 'spoof'\n",
    "        })\n",
    "\n",
    "# Create new protocol DataFrame\n",
    "print(\"Creating final DataFrame...\")\n",
    "new_protocol_vocoded = pd.DataFrame(new_entries)\n",
    "print(f\"Original protocol size: {len(df_concat)}\")\n",
    "print(f\"New protocol size: {len(new_protocol_vocoded)}\")\n",
    "\n",
    "# Concatemate df_concat and new_protocol_vocoded\n",
    "df_concat3 = pd.concat([df_concat, new_protocol_vocoded], axis=0)\n",
    "\n",
    "# save df_concat3\n",
    "df_concat3.to_csv('new_protocol_trim_vocoded_v2.txt', sep=' ', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subset  label   \n",
       "dev     bonafide     9787\n",
       "        spoof       16185\n",
       "eval    bonafide    67977\n",
       "        spoof       66464\n",
       "train   bonafide    19700\n",
       "        spoof       68046\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_concat3 = pd.read_csv('new_protocol_trim_vocoded_v2.txt', sep=' ', header=None)\n",
    "df_concat3.columns = ['file_name', 'subset', 'label']\n",
    "\n",
    "# Check number of spoof and bonafide samples each subset\n",
    "df_concat3.groupby(['subset', 'label']).size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
